{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Imports-&amp;-Inits\" data-toc-modified-id=\"Imports-&amp;-Inits-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Imports &amp; Inits</a></span></li><li><span><a href=\"#Data-Preparation\" data-toc-modified-id=\"Data-Preparation-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Data Preparation</a></span></li><li><span><a href=\"#Model\" data-toc-modified-id=\"Model-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Model</a></span></li><li><span><a href=\"#Training\" data-toc-modified-id=\"Training-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Training</a></span></li><li><span><a href=\"#Testing\" data-toc-modified-id=\"Testing-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Testing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Ignite-Testing\" data-toc-modified-id=\"Ignite-Testing-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Ignite Testing</a></span></li><li><span><a href=\"#NLPBook-Testing\" data-toc-modified-id=\"NLPBook-Testing-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>NLPBook Testing</a></span></li></ul></li><li><span><a href=\"#Inference\" data-toc-modified-id=\"Inference-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Inference</a></span><ul class=\"toc-item\"><li><span><a href=\"#Single-Inference\" data-toc-modified-id=\"Single-Inference-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Single Inference</a></span></li><li><span><a href=\"#TopK-Inference\" data-toc-modified-id=\"TopK-Inference-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>TopK Inference</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Surname Classifier with MLP\n",
    "\n",
    "Classifying surnames based on national origin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Inits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T23:51:59.462639Z",
     "start_time": "2019-03-25T23:51:59.450717Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T23:52:00.318082Z",
     "start_time": "2019-03-25T23:51:59.464136Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T23:52:00.365924Z",
     "start_time": "2019-03-25T23:52:00.319980Z"
    }
   },
   "outputs": [],
   "source": [
    "from ignite.engine import Events, create_supervised_evaluator\n",
    "from ignite.metrics import Accuracy, Loss\n",
    "from ignite.contrib.handlers import ProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T23:52:00.382517Z",
     "start_time": "2019-03-25T23:52:00.367178Z"
    }
   },
   "outputs": [],
   "source": [
    "from surname.dataset import CNNDataset\n",
    "# from surname.model import CNNClassifer\n",
    "from surname.trainer import Trainer\n",
    "from surname.args import cnn_args as args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T23:52:00.395676Z",
     "start_time": "2019-03-25T23:52:00.383472Z"
    }
   },
   "outputs": [],
   "source": [
    "path = Path('../data/surnames')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T23:52:00.413566Z",
     "start_time": "2019-03-25T23:52:00.396629Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(batch_size=64, checkpointer_name='cnn_classifier', checkpointer_prefix='surname', cw_file='class_weights.pt', device='cuda:3', dropout_p=0.1, early_stopping_criteria=5, hidden_dim=100, learning_rate=0.001, model_dir='models', num_channels=256, num_epochs=100, proc_dataset_csv='surnames_with_splits.csv', raw_dataset_csv='surnames.csv', save_every=2, save_total=5, train_proportion=0.7, vectorizer_fname='cnn_vectorizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "work_dir = path/'work_dir'\n",
    "surnames_csv = path/args.proc_dataset_csv\n",
    "vectorizer_path = work_dir/args.vectorizer_fname\n",
    "cw_file = work_dir/args.cw_file\n",
    "\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T23:52:00.438351Z",
     "start_time": "2019-03-25T23:52:00.414620Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10980"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(surnames_csv)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T23:52:00.451152Z",
     "start_time": "2019-03-25T23:52:00.439589Z"
    }
   },
   "outputs": [],
   "source": [
    "is_load = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T23:52:00.465631Z",
     "start_time": "2019-03-25T23:52:00.452309Z"
    }
   },
   "outputs": [],
   "source": [
    "if not is_load:\n",
    "  train_ds = ProjectDataset.load_data_and_create_vectorizer(df.loc[df['split'] == 'train'])\n",
    "  train_ds.save_vectorizer(vectorizer_path)\n",
    "  vectorizer = train_ds.get_vectorizer()\n",
    "  class_counts = df['nationality'].value_counts().to_dict()\n",
    "  sorted_counts = sorted(class_counts.items(), key=lambda x: vectorizer.nationality_vocab.lookup_token(x[0]))\n",
    "  freq = [count for _, count in sorted_counts]\n",
    "  class_weights = 1.0/torch.tensor(freq, dtype=torch.float32)\n",
    "  torch.save(class_weights, cw_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T23:52:00.487681Z",
     "start_time": "2019-03-25T23:52:00.466661Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7680, 1640, 1660)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = df.loc[df['split'] == 'train']\n",
    "train_ds = CNNDataset.load_data_and_vectorizer(train_df, vectorizer_path)\n",
    "vectorizer = train_ds.get_vectorizer()\n",
    "class_weights = torch.load(cw_file)\n",
    "train_dl = DataLoader(train_ds, args.batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "val_df = df.loc[df['split'] == 'val']\n",
    "val_ds = CNNDataset.load_data_and_vectorizer(val_df, vectorizer_path)\n",
    "val_dl = DataLoader(val_ds, args.batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "test_df = df.loc[df['split'] == 'test']\n",
    "test_ds = CNNDataset.load_data_and_vectorizer(test_df, vectorizer_path)\n",
    "test_dl = DataLoader(test_ds, args.batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "len(train_dl.dataset), len(val_dl.dataset), len(test_dl.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T23:52:20.714525Z",
     "start_time": "2019-03-25T23:52:20.697473Z"
    }
   },
   "outputs": [],
   "source": [
    "class CNNClassifier(nn.Module):\n",
    "  def __init__(self, initial_num_channels, num_classes, num_channels):\n",
    "    super(CNNClassifier, self).__init__()\n",
    "\n",
    "    self.convnet = nn.Sequential(\n",
    "          nn.Conv1d(in_channels=initial_num_channels, out_channels=num_channels, kernel_size=3),\n",
    "          nn.ELU(),\n",
    "          nn.Conv1d(in_channels=initial_num_channels, out_channels=num_channels, kernel_size=3,\n",
    "            stride=2),\n",
    "          nn.ELU(),\n",
    "          nn.Conv1d(in_channels=initial_num_channels, out_channels=num_channels, kernel_size=3,\n",
    "            stride=2),\n",
    "          nn.ELU(),\n",
    "          nn.Conv1d(in_channels=initial_num_channels, out_channels=num_channels, kernel_size=3,\n",
    "            stride=2),\n",
    "          nn.ELU()\n",
    "        )\n",
    "    self.fc = nn.Linear(num_channels, num_classes)\n",
    "    self.softmax = nn.Softmax()\n",
    "\n",
    "  def forward(self, x_in, apply_softmax=False):\n",
    "    y_out = self.convnet(x_in).squeeze(dim=2)\n",
    "    y_out = self.fc(y_out)\n",
    "\n",
    "    if apply_softmax:\n",
    "      y_out = self.softmax(y_out)\n",
    "\n",
    "    return y_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T23:53:44.843507Z",
     "start_time": "2019-03-25T23:53:39.047480Z"
    }
   },
   "outputs": [],
   "source": [
    "classifier = CNNClassifier(initial_num_channels=len(vectorizer.surname_vocab), num_classes=len(vectorizer.nationality_vocab), num_channels=args.num_channels)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
    "class_weights = class_weights.to(args.device)\n",
    "loss_func = nn.CrossEntropyLoss(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T23:53:44.843507Z",
     "start_time": "2019-03-25T23:53:39.047480Z"
    }
   },
   "outputs": [],
   "source": [
    "pbar = ProgressBar(persist=True)\n",
    "metrics = {'accuracy': Accuracy(), 'loss': Loss(loss_func)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T21:06:28.598618Z",
     "start_time": "2019-03-24T21:05:30.528208Z"
    }
   },
   "outputs": [],
   "source": [
    "surname_trainer = Trainer(classifier, optimizer, loss_func, train_dl, val_dl, work_dir, args, pbar, metrics)\n",
    "surname_trainer.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ignite Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T21:09:03.535868Z",
     "start_time": "2019-03-24T21:09:00.865314Z"
    }
   },
   "outputs": [],
   "source": [
    "args.device = 'cpu'\n",
    "classifier = Classifier(input_dim=len(vectorizer.surname_vocab), hidden_dim=args.hidden_dim,\\\n",
    "                        output_dim=len(vectorizer.nationality_vocab))\n",
    "# state_dict = torch.load(work_dir/args.model_dir/'surname_classifier_24.pth')\n",
    "state_dict = torch.load(work_dir/'surname_classifier_model.pth')\n",
    "classifier.load_state_dict(state_dict)\n",
    "\n",
    "class_weights = class_weights.to(args.device)\n",
    "loss_func = nn.CrossEntropyLoss(class_weights)\n",
    "metrics = {'accuracy': Accuracy(), 'loss': Loss(loss_func)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T21:09:05.111056Z",
     "start_time": "2019-03-24T21:09:05.094048Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluator = create_supervised_evaluator(classifier, metrics=metrics)\n",
    "\n",
    "@evaluator.on(Events.COMPLETED)\n",
    "def log_testing_results(engine):\n",
    "  metrics = engine.state.metrics\n",
    "  print(f\"Test loss: {metrics['loss']:0.3f}\")\n",
    "  print(f\"Test accuracy: {metrics['accuracy']:0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T21:09:05.405588Z",
     "start_time": "2019-03-24T21:09:05.112282Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluator.run(test_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLPBook Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T21:09:08.093189Z",
     "start_time": "2019-03-24T21:09:08.079233Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(y_pred, y_target):\n",
    "  _, y_pred_indices = y_pred.max(dim=1)\n",
    "  n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "  return n_correct / len(y_pred_indices) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T21:09:08.377452Z",
     "start_time": "2019-03-24T21:09:08.094495Z"
    }
   },
   "outputs": [],
   "source": [
    "running_loss = 0.\n",
    "running_acc = 0.\n",
    "\n",
    "classifier.eval()\n",
    "for i, batch in enumerate(test_dl):\n",
    "  x,y = batch\n",
    "  y_pred = classifier(x_in=x.float())\n",
    "  \n",
    "  loss = loss_func(y_pred, y)\n",
    "  loss_t = loss.item()\n",
    "  running_loss += (loss_t-running_loss)/(i+1)\n",
    "  \n",
    "  acc_t = compute_accuracy(y_pred, y)\n",
    "  running_acc += (acc_t-running_acc)/(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T21:09:08.394199Z",
     "start_time": "2019-03-24T21:09:08.378947Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Test loss: {running_loss:0.3f}\")\n",
    "print(f\"Test acc: {running_acc:0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T21:09:11.375267Z",
     "start_time": "2019-03-24T21:09:11.361526Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_natinoality(surname, classifier, vectorizer):\n",
    "  \"\"\"\n",
    "    Predict the nationality from a new surname\n",
    "    \n",
    "    Args:\n",
    "      surname: the surname to classify\n",
    "      classifier: an instance of the classifier\n",
    "      vectorizer: the corresponding vectorizer\n",
    "      \n",
    "    Returns:\n",
    "      a dictionary with most likely natinoality and its probability\n",
    "  \"\"\"\n",
    "  vectorized_surname = vectorizer.vectorize(surname)\n",
    "  vectorized_surname = torch.tensor(vectorized_surname).view(1,-1)\n",
    "  result = classifier(vectorized_surname, apply_softmax=True)\n",
    "  \n",
    "  probability_values, indices = result.max(dim=1)\n",
    "  idx = indices.item()\n",
    "  \n",
    "  predicted_nationality = vectorizer.nationality_vocab.lookup_idx(idx)\n",
    "  probability_value = probability_values.item()\n",
    "  \n",
    "  return {'nationality': predicted_nationality, 'probability': probability_value}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T21:09:20.066679Z",
     "start_time": "2019-03-24T21:09:11.376549Z"
    }
   },
   "outputs": [],
   "source": [
    "new_surname = input(\"Enter a surname to classify: \")\n",
    "prediction = predict_natinoality(new_surname, classifier, vectorizer)\n",
    "print(f\"{new_surname} -> {prediction['nationality']} p={prediction['probability']:0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TopK Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T21:09:27.376695Z",
     "start_time": "2019-03-24T21:09:27.361009Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_topk_nationality(name, classifier, vectorizer, k=5):\n",
    "  vectorized_name = vectorizer.vectorize(name)\n",
    "  vectorized_name = torch.tensor(vectorized_name).view(1, -1)\n",
    "  prediction_vector = classifier(vectorized_name, apply_softmax=True)\n",
    "  probability_values, indices = torch.topk(prediction_vector, k=k)\n",
    "\n",
    "  # returned size is 1,k\n",
    "  probability_values = probability_values.detach().numpy()[0]\n",
    "  indices = indices.detach().numpy()[0]\n",
    "\n",
    "  results = []\n",
    "  for prob_value, idx in zip(probability_values, indices):\n",
    "      nationality = vectorizer.nationality_vocab.lookup_idx(idx)\n",
    "      results.append({'nationality': nationality, \n",
    "                      'probability': prob_value})\n",
    "\n",
    "  return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T21:09:32.568318Z",
     "start_time": "2019-03-24T21:09:28.121230Z"
    }
   },
   "outputs": [],
   "source": [
    "new_surname = input(\"Enter a surname to classify: \")\n",
    "classifier = classifier.to(\"cpu\")\n",
    "\n",
    "k = int(input(\"How many of the top predictions to see? \"))\n",
    "if k > len(vectorizer.nationality_vocab):\n",
    "  print(\"Sorry! That's more than the # of nationalities we have.. defaulting you to max size :)\")\n",
    "  k = len(vectorizer.nationality_vocab)\n",
    "    \n",
    "predictions = predict_topk_nationality(new_surname, classifier, vectorizer, k=k)\n",
    "\n",
    "print(\"Top {} predictions:\".format(k))\n",
    "print(\"===================\")\n",
    "for prediction in predictions:\n",
    "  print(f\"{new_surname} -> {prediction['nationality']} p={prediction['probability']:0.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
