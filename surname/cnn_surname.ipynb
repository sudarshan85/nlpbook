{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Imports-&amp;-Inits\" data-toc-modified-id=\"Imports-&amp;-Inits-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Imports &amp; Inits</a></span></li><li><span><a href=\"#Data-Preparation\" data-toc-modified-id=\"Data-Preparation-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Data Preparation</a></span></li><li><span><a href=\"#Model\" data-toc-modified-id=\"Model-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Model</a></span></li><li><span><a href=\"#Training\" data-toc-modified-id=\"Training-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Training</a></span></li><li><span><a href=\"#Testing\" data-toc-modified-id=\"Testing-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Testing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Ignite-Testing\" data-toc-modified-id=\"Ignite-Testing-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Ignite Testing</a></span></li><li><span><a href=\"#NLPBook-Testing\" data-toc-modified-id=\"NLPBook-Testing-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>NLPBook Testing</a></span></li></ul></li><li><span><a href=\"#Inference\" data-toc-modified-id=\"Inference-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Inference</a></span><ul class=\"toc-item\"><li><span><a href=\"#Single-Inference\" data-toc-modified-id=\"Single-Inference-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Single Inference</a></span></li><li><span><a href=\"#TopK-Inference\" data-toc-modified-id=\"TopK-Inference-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>TopK Inference</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Surname Classifier with MLP\n",
    "\n",
    "Classifying surnames based on national origin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Inits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T02:00:17.244984Z",
     "start_time": "2019-03-26T02:00:17.232741Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T02:00:17.732428Z",
     "start_time": "2019-03-26T02:00:17.246346Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pdb\n",
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T02:00:17.774994Z",
     "start_time": "2019-03-26T02:00:17.734013Z"
    }
   },
   "outputs": [],
   "source": [
    "from ignite.engine import Events, create_supervised_evaluator\n",
    "from ignite.metrics import Accuracy, Loss\n",
    "from ignite.contrib.handlers import ProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T02:00:17.789798Z",
     "start_time": "2019-03-26T02:00:17.776192Z"
    }
   },
   "outputs": [],
   "source": [
    "from surname.dataset import CNNDataset\n",
    "from surname.model import CNNClassifier\n",
    "from surname.trainer import Trainer\n",
    "from surname.args import cnn_args as args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T02:00:17.802559Z",
     "start_time": "2019-03-26T02:00:17.790782Z"
    }
   },
   "outputs": [],
   "source": [
    "path = Path('../data/surnames')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T02:00:17.820045Z",
     "start_time": "2019-03-26T02:00:17.803466Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(batch_size=64, checkpointer_name='cnn_classifier', checkpointer_prefix='surname', cw_file='class_weights.pt', device='cuda:3', dropout_p=0.1, early_stopping_criteria=5, hidden_dim=100, learning_rate=0.001, model_dir='models', num_channels=256, num_epochs=100, proc_dataset_csv='surnames_with_splits.csv', raw_dataset_csv='surnames.csv', save_every=2, save_total=5, train_proportion=0.7, vectorizer_fname='cnn_vectorizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "work_dir = path/'work_dir'\n",
    "surnames_csv = path/args.proc_dataset_csv\n",
    "vectorizer_path = work_dir/args.vectorizer_fname\n",
    "cw_file = work_dir/args.cw_file\n",
    "\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T02:00:17.843396Z",
     "start_time": "2019-03-26T02:00:17.820924Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10980"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(surnames_csv)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T02:00:17.855862Z",
     "start_time": "2019-03-26T02:00:17.844630Z"
    }
   },
   "outputs": [],
   "source": [
    "is_load = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T02:00:17.870034Z",
     "start_time": "2019-03-26T02:00:17.856972Z"
    }
   },
   "outputs": [],
   "source": [
    "if not is_load:\n",
    "  train_ds = ProjectDataset.load_data_and_create_vectorizer(df.loc[df['split'] == 'train'])\n",
    "  train_ds.save_vectorizer(vectorizer_path)\n",
    "  vectorizer = train_ds.get_vectorizer()\n",
    "  class_counts = df['nationality'].value_counts().to_dict()\n",
    "  sorted_counts = sorted(class_counts.items(), key=lambda x: vectorizer.nationality_vocab.lookup_token(x[0]))\n",
    "  freq = [count for _, count in sorted_counts]\n",
    "  class_weights = 1.0/torch.tensor(freq, dtype=torch.float32)\n",
    "  torch.save(class_weights, cw_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T02:00:17.892093Z",
     "start_time": "2019-03-26T02:00:17.871033Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7680, 1640, 1660)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = df.loc[df['split'] == 'train']\n",
    "train_ds = CNNDataset.load_data_and_vectorizer(train_df, vectorizer_path)\n",
    "vectorizer = train_ds.get_vectorizer()\n",
    "class_weights = torch.load(cw_file)\n",
    "train_dl = DataLoader(train_ds, args.batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "val_df = df.loc[df['split'] == 'val']\n",
    "val_ds = CNNDataset.load_data_and_vectorizer(val_df, vectorizer_path)\n",
    "val_dl = DataLoader(val_ds, args.batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "data_bundle = {\n",
    "  'train_dl': train_dl,\n",
    "  'val_dl': val_dl\n",
    "}\n",
    "\n",
    "test_df = df.loc[df['split'] == 'test']\n",
    "test_ds = CNNDataset.load_data_and_vectorizer(test_df, vectorizer_path)\n",
    "test_dl = DataLoader(test_ds, args.batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "len(train_dl.dataset), len(val_dl.dataset), len(test_dl.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T02:00:23.615871Z",
     "start_time": "2019-03-26T02:00:17.893001Z"
    }
   },
   "outputs": [],
   "source": [
    "classifier = CNNClassifier(initial_num_channels=len(vectorizer.surname_vocab),\\\n",
    "                           num_classes=len(vectorizer.nationality_vocab),\\\n",
    "                           num_channels=args.num_channels)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
    "class_weights = class_weights.to(args.device)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='min', factor=0.5, patience=1)\n",
    "loss_func = nn.CrossEntropyLoss(class_weights)\n",
    "\n",
    "model_bundle = {\n",
    "  'module': classifier,\n",
    "  'optimizer' : optimizer,\n",
    "  'scheduler': scheduler,\n",
    "  'loss_fn': loss_func\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T02:00:23.704205Z",
     "start_time": "2019-03-26T02:00:23.617646Z"
    }
   },
   "outputs": [],
   "source": [
    "pbar = ProgressBar(persist=True)\n",
    "metrics = {'accuracy': Accuracy(), 'loss': Loss(loss_func)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T02:00:54.053045Z",
     "start_time": "2019-03-26T02:00:23.705674Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/100]: [120/120] 100%|██████████, loss=2.39e+00 [00:01<00:00]\n",
      "Epoch [2/100]: [16/120]  13%|█▎        , loss=2.13e+00 [00:00<00:01]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Training - Loss: 1.943, Accuracy: 0.446\n",
      "Validation - Loss: 2.090, Accuracy: 0.411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/100]: [120/120] 100%|██████████, loss=1.90e+00 [00:01<00:00]\n",
      "Epoch [3/100]: [14/120]  12%|█▏        , loss=1.42e+00 [00:00<00:01]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n",
      "Training - Loss: 1.656, Accuracy: 0.418\n",
      "Validation - Loss: 1.975, Accuracy: 0.365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [3/100]: [120/120] 100%|██████████, loss=1.67e+00 [00:01<00:00]\n",
      "Epoch [4/100]: [16/120]  13%|█▎        , loss=1.47e+00 [00:00<00:01]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\n",
      "Training - Loss: 1.509, Accuracy: 0.468\n",
      "Validation - Loss: 1.867, Accuracy: 0.425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [4/100]: [120/120] 100%|██████████, loss=1.50e+00 [00:01<00:00]\n",
      "Epoch [5/100]: [13/120]  11%|█         , loss=1.29e+00 [00:00<00:01]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\n",
      "Training - Loss: 1.337, Accuracy: 0.525\n",
      "Validation - Loss: 1.876, Accuracy: 0.469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [5/100]: [120/120] 100%|██████████, loss=1.41e+00 [00:01<00:00]\n",
      "Epoch [6/100]: [16/120]  13%|█▎        , loss=1.31e+00 [00:00<00:01]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5\n",
      "Training - Loss: 1.245, Accuracy: 0.533\n",
      "Validation - Loss: 1.870, Accuracy: 0.475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [6/100]: [120/120] 100%|██████████, loss=1.18e+00 [00:01<00:00]\n",
      "Epoch [7/100]: [14/120]  12%|█▏        , loss=9.38e-01 [00:00<00:01]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6\n",
      "Training - Loss: 1.016, Accuracy: 0.563\n",
      "Validation - Loss: 1.747, Accuracy: 0.492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [7/100]: [120/120] 100%|██████████, loss=1.09e+00 [00:01<00:00]\n",
      "Epoch [8/100]: [15/120]  12%|█▎        , loss=1.13e+00 [00:00<00:01]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7\n",
      "Training - Loss: 0.948, Accuracy: 0.623\n",
      "Validation - Loss: 1.795, Accuracy: 0.544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [8/100]: [120/120] 100%|██████████, loss=1.02e+00 [00:01<00:00]\n",
      "Epoch [9/100]: [13/120]  11%|█         , loss=9.38e-01 [00:00<00:01]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8\n",
      "Training - Loss: 0.916, Accuracy: 0.624\n",
      "Validation - Loss: 1.801, Accuracy: 0.541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [9/100]: [120/120] 100%|██████████, loss=8.81e-01 [00:01<00:00]\n",
      "Epoch [10/100]: [16/120]  13%|█▎        , loss=8.29e-01 [00:00<00:01]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9\n",
      "Training - Loss: 0.791, Accuracy: 0.634\n",
      "Validation - Loss: 1.805, Accuracy: 0.544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [10/100]: [120/120] 100%|██████████, loss=8.46e-01 [00:01<00:00]\n",
      "Epoch [11/100]: [13/120]  11%|█         , loss=6.40e-01 [00:00<00:01]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10\n",
      "Training - Loss: 0.754, Accuracy: 0.676\n",
      "Validation - Loss: 1.877, Accuracy: 0.573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [11/100]: [120/120] 100%|██████████, loss=7.58e-01 [00:01<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11\n",
      "Training - Loss: 0.712, Accuracy: 0.663\n",
      "Validation - Loss: 1.865, Accuracy: 0.565\n"
     ]
    }
   ],
   "source": [
    "surname_trainer = Trainer(model_bundle, data_bundle, work_dir, args, pbar, metrics)\n",
    "surname_trainer.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ignite Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T02:02:30.540600Z",
     "start_time": "2019-03-26T02:02:30.507102Z"
    }
   },
   "outputs": [],
   "source": [
    "args.device = 'cpu'\n",
    "classifier = CNNClassifier(initial_num_channels=len(vectorizer.surname_vocab),\\\n",
    "                           num_classes=len(vectorizer.nationality_vocab),\\\n",
    "                           num_channels=args.num_channels)# state_dict = torch.load(work_dir/args.model_dir/'surname_classifier_24.pth')\n",
    "state_dict = torch.load(work_dir/'surname_cnn_classifier.pth')\n",
    "classifier.load_state_dict(state_dict)\n",
    "\n",
    "class_weights = class_weights.to(args.device)\n",
    "loss_func = nn.CrossEntropyLoss(class_weights)\n",
    "metrics = {'accuracy': Accuracy(), 'loss': Loss(loss_func)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T02:02:35.019320Z",
     "start_time": "2019-03-26T02:02:35.005017Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluator = create_supervised_evaluator(classifier, metrics=metrics)\n",
    "\n",
    "@evaluator.on(Events.COMPLETED)\n",
    "def log_testing_results(engine):\n",
    "  metrics = engine.state.metrics\n",
    "  print(f\"Test loss: {metrics['loss']:0.3f}\")\n",
    "  print(f\"Test accuracy: {metrics['accuracy']:0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T02:02:35.688251Z",
     "start_time": "2019-03-26T02:02:35.020596Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.696\n",
      "Test accuracy: 0.571\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ignite.engine.engine.State at 0x7f3bc54a5ac8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.run(test_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLPBook Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T02:02:38.929948Z",
     "start_time": "2019-03-26T02:02:38.915626Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(y_pred, y_target):\n",
    "  _, y_pred_indices = y_pred.max(dim=1)\n",
    "  n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "  return n_correct / len(y_pred_indices) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T02:02:39.579735Z",
     "start_time": "2019-03-26T02:02:38.939140Z"
    }
   },
   "outputs": [],
   "source": [
    "running_loss = 0.\n",
    "running_acc = 0.\n",
    "\n",
    "classifier.eval()\n",
    "for i, batch in enumerate(test_dl):\n",
    "  x,y = batch\n",
    "  y_pred = classifier(x_in=x.float())\n",
    "  \n",
    "  loss = loss_func(y_pred, y)\n",
    "  loss_t = loss.item()\n",
    "  running_loss += (loss_t-running_loss)/(i+1)\n",
    "  \n",
    "  acc_t = compute_accuracy(y_pred, y)\n",
    "  running_acc += (acc_t-running_acc)/(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T02:02:39.600538Z",
     "start_time": "2019-03-26T02:02:39.581485Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.751\n",
      "Test acc: 57.250\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test loss: {running_loss:0.3f}\")\n",
    "print(f\"Test acc: {running_acc:0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T02:04:04.399436Z",
     "start_time": "2019-03-26T02:04:04.258254Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_natinoality(surname, classifier, vectorizer):\n",
    "  \"\"\"\n",
    "    Predict the nationality from a new surname\n",
    "    \n",
    "    Args:\n",
    "      surname: the surname to classify\n",
    "      classifier: an instance of the classifier\n",
    "      vectorizer: the corresponding vectorizer\n",
    "      \n",
    "    Returns:\n",
    "      a dictionary with most likely natinoality and its probability\n",
    "  \"\"\"\n",
    "  vectorized_surname = vectorizer.vectorize(surname)\n",
    "  vectorized_surname = torch.tensor(vectorized_surname).unsqueeze(0)\n",
    "  result = classifier(vectorized_surname, apply_softmax=True)\n",
    "  \n",
    "  probability_values, indices = result.max(dim=1)\n",
    "  idx = indices.item()\n",
    "  \n",
    "  predicted_nationality = vectorizer.nationality_vocab.lookup_idx(idx)\n",
    "  probability_value = probability_values.item()\n",
    "  \n",
    "  return {'nationality': predicted_nationality, 'probability': probability_value}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T02:04:14.366035Z",
     "start_time": "2019-03-26T02:04:11.733787Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a surname to classify: Kumar\n",
      "Kumar -> Arabic p=0.58\n"
     ]
    }
   ],
   "source": [
    "new_surname = input(\"Enter a surname to classify: \")\n",
    "prediction = predict_natinoality(new_surname, classifier, vectorizer)\n",
    "print(f\"{new_surname} -> {prediction['nationality']} p={prediction['probability']:0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TopK Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T02:05:23.116906Z",
     "start_time": "2019-03-26T02:05:22.974780Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_topk_nationality(name, classifier, vectorizer, k=5):\n",
    "  vectorized_name = vectorizer.vectorize(name)\n",
    "  vectorized_name = torch.tensor(vectorized_name).unsqueeze(0)\n",
    "  prediction_vector = classifier(vectorized_name, apply_softmax=True)\n",
    "  probability_values, indices = torch.topk(prediction_vector, k=k)\n",
    "\n",
    "  # returned size is 1,k\n",
    "  probability_values = probability_values.detach().numpy()[0]\n",
    "  indices = indices.detach().numpy()[0]\n",
    "\n",
    "  results = []\n",
    "  for prob_value, idx in zip(probability_values, indices):\n",
    "      nationality = vectorizer.nationality_vocab.lookup_idx(idx)\n",
    "      results.append({'nationality': nationality, \n",
    "                      'probability': prob_value})\n",
    "\n",
    "  return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T02:05:49.782279Z",
     "start_time": "2019-03-26T02:05:46.683198Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a surname to classify: romanoff\n",
      "How many of the top predictions to see? 5\n",
      "Top 5 predictions:\n",
      "===================\n",
      "romanoff -> Russian p=0.71\n",
      "romanoff -> Czech p=0.17\n",
      "romanoff -> Polish p=0.07\n",
      "romanoff -> German p=0.03\n",
      "romanoff -> English p=0.02\n"
     ]
    }
   ],
   "source": [
    "new_surname = input(\"Enter a surname to classify: \")\n",
    "classifier = classifier.to(\"cpu\")\n",
    "\n",
    "k = int(input(\"How many of the top predictions to see? \"))\n",
    "if k > len(vectorizer.nationality_vocab):\n",
    "  print(\"Sorry! That's more than the # of nationalities we have.. defaulting you to max size :)\")\n",
    "  k = len(vectorizer.nationality_vocab)\n",
    "    \n",
    "predictions = predict_topk_nationality(new_surname, classifier, vectorizer, k=k)\n",
    "\n",
    "print(\"Top {} predictions:\".format(k))\n",
    "print(\"===================\")\n",
    "for prediction in predictions:\n",
    "  print(f\"{new_surname} -> {prediction['nationality']} p={prediction['probability']:0.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
