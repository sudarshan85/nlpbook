{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Imports-&amp;-Inits\" data-toc-modified-id=\"Imports-&amp;-Inits-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Imports &amp; Inits</a></span></li><li><span><a href=\"#Functions\" data-toc-modified-id=\"Functions-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Functions</a></span></li><li><span><a href=\"#Full-Dataset-Preprocessing\" data-toc-modified-id=\"Full-Dataset-Preprocessing-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Full Dataset Preprocessing</a></span></li><li><span><a href=\"#Data-Preparation\" data-toc-modified-id=\"Data-Preparation-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Data Preparation</a></span></li><li><span><a href=\"#Model\" data-toc-modified-id=\"Model-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Model</a></span></li><li><span><a href=\"#Training\" data-toc-modified-id=\"Training-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Training</a></span></li><li><span><a href=\"#Testing\" data-toc-modified-id=\"Testing-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Testing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Ignite-Testing\" data-toc-modified-id=\"Ignite-Testing-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Ignite Testing</a></span></li><li><span><a href=\"#NLPBook-Testing\" data-toc-modified-id=\"NLPBook-Testing-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>NLPBook Testing</a></span></li><li><span><a href=\"#Predict-Rating\" data-toc-modified-id=\"Predict-Rating-7.3\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;</span>Predict Rating</a></span></li><li><span><a href=\"#Interpretablity\" data-toc-modified-id=\"Interpretablity-7.4\"><span class=\"toc-item-num\">7.4&nbsp;&nbsp;</span>Interpretablity</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yelp Review Classifier from NLP Book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yelp restaurant review binary classifier problem from NLP with PyTorch book. This uses the Ignite framework for training the model. The details of the problem can be found at page 57 of the book. [Here](https://nbviewer.jupyter.org/github/joosthub/PyTorchNLPBook/blob/master/chapters/chapter_3/3_5_Classifying_Yelp_Review_Sentiment.ipynb) is the notebook for training. I've made some changes in the code, refactoring the notebook code into modules.\n",
    "\n",
    "There is already a preprocessed \"lite\" dataset file which has 10\\% of the data. The code was already tested on the lite version before processing the full version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Inits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T19:23:42.508906Z",
     "start_time": "2019-03-23T19:23:42.427289Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T19:23:43.495039Z",
     "start_time": "2019-03-23T19:23:42.510152Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import pdb\n",
    "import re\n",
    "\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T19:23:43.541209Z",
     "start_time": "2019-03-23T19:23:43.496746Z"
    }
   },
   "outputs": [],
   "source": [
    "from ignite.engine import Events, create_supervised_evaluator\n",
    "from ignite.metrics import Accuracy, Loss\n",
    "from ignite.contrib.handlers import ProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T19:23:43.556153Z",
     "start_time": "2019-03-23T19:23:43.542395Z"
    }
   },
   "outputs": [],
   "source": [
    "# imports from my modules\n",
    "from yelp.dataset import ProjectDataset\n",
    "from yelp.trainer import YelpTrainer\n",
    "from yelp.model import Classifier\n",
    "from yelp.args import args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T19:23:43.568959Z",
     "start_time": "2019-03-23T19:23:43.557153Z"
    }
   },
   "outputs": [],
   "source": [
    "path = Path('./data/yelp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T19:23:43.582180Z",
     "start_time": "2019-03-23T19:23:43.569924Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "  text = text.lower()\n",
    "  text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
    "  text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
    "  return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Full Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T19:20:38.308108Z",
     "start_time": "2019-03-23T19:20:34.744150Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_reviews = pd.read_csv(path/args.raw_train_csv, header=None, names=['rating', 'review'])\n",
    "train_reviews = train_reviews[~pd.isnull(train_reviews['review'])]\n",
    "\n",
    "test_reviews = pd.read_csv(path/args.raw_test_csv, header=None, names=['rating', 'review'])\n",
    "test_reviews = test_reviews[~pd.isnull(test_reviews['review'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T19:20:38.339532Z",
     "start_time": "2019-03-23T19:20:38.309448Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T19:20:38.356514Z",
     "start_time": "2019-03-23T19:20:38.340865Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T19:21:23.899868Z",
     "start_time": "2019-03-23T19:20:38.357773Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# splitting train by rating\n",
    "by_rating = defaultdict(list)\n",
    "for _, row in train_reviews.iterrows():\n",
    "    by_rating[row['rating']].append(row.to_dict())\n",
    "\n",
    "# create split data\n",
    "final_list = []\n",
    "\n",
    "for _, item_list in sorted(by_rating.items()):\n",
    "  np.random.shuffle(item_list)\n",
    "  n_total = len(item_list)\n",
    "  n_train = int(args.train_proportion * n_total)\n",
    "  n_val = int((1-args.train_proportion) * n_total)\n",
    "  \n",
    "  # give data point a split attribute\n",
    "  for item in item_list[:n_train]:\n",
    "    item['split'] = 'train'\n",
    "  \n",
    "  for item in item_list[n_train:n_train+n_val]:\n",
    "    item['split'] = 'val'\n",
    "    \n",
    "  # add to final list\n",
    "  final_list.extend(item_list)\n",
    "\n",
    "# add test split\n",
    "for _, row in test_reviews.iterrows():\n",
    "  row_dict = row.to_dict()\n",
    "  row_dict['split'] = 'test'\n",
    "  final_list.append(row_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T19:21:25.014622Z",
     "start_time": "2019-03-23T19:21:23.902250Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# write split data to file\n",
    "final_reviews = pd.DataFrame(final_list)\n",
    "final_reviews.split.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T19:21:25.034076Z",
     "start_time": "2019-03-23T19:21:25.015948Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "final_reviews['review'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T19:21:57.800529Z",
     "start_time": "2019-03-23T19:21:25.035360Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "final_reviews['review'] = final_reviews['review'].apply(preprocess_text)\n",
    "final_reviews['rating'] = final_reviews['rating'].apply({1: 'negative', 2: 'positive'}.get)\n",
    "final_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T19:22:04.033288Z",
     "start_time": "2019-03-23T19:21:57.801843Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "final_reviews.to_csv(path/args.full_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T19:23:46.492231Z",
     "start_time": "2019-03-23T19:23:46.480680Z"
    }
   },
   "outputs": [],
   "source": [
    "is_lite = True\n",
    "is_load = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T19:23:46.509570Z",
     "start_time": "2019-03-23T19:23:46.493410Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(batch_size=1024, checkpointer_name='classifier', checkpointer_prefix='yelp', device='cuda:3', early_stopping_criteria=5, frequency_cutoff=25, full_dir='models/full', full_file='reviews_with_splits_full.csv', learning_rate=0.001, lite_dir='models/lite', lite_file='reviews_with_splits_lite.csv', num_epochs=100, raw_test_csv='raw_test.csv', raw_train_csv='raw_train.csv', save_dir=PosixPath('data/yelp/models/lite'), save_every=2, save_total=5, train_proportion=0.7, vectorizer_fname='vectorizer.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if is_lite:\n",
    "  scratch = path/args.lite_dir\n",
    "  review_csv = path/args.lite_file\n",
    "else:\n",
    "  scratch = path/args.full_dir\n",
    "  review_csv = path/args.full_file\n",
    "\n",
    "vectorizer_path = scratch/args.vectorizer_fname\n",
    "args.save_dir = scratch\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T19:23:46.860425Z",
     "start_time": "2019-03-23T19:23:46.510709Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(review_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run only once for creating vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T19:23:48.631229Z",
     "start_time": "2019-03-23T19:23:48.617051Z"
    }
   },
   "outputs": [],
   "source": [
    "if not is_load:\n",
    "  train_ds = ProjectDataset.load_data_and_create_vectorizer(df.loc[df['split'] == 'train'])\n",
    "  train_ds.save_vectorizer(vectorizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T19:23:49.465861Z",
     "start_time": "2019-03-23T19:23:49.432795Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = df.loc[df['split'] == 'train']\n",
    "train_ds = ProjectDataset.load_data_and_vectorizer(train_df, vectorizer_path)\n",
    "vectorizer = train_ds.get_vectorizer()\n",
    "train_dl = DataLoader(train_ds, args.batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "val_df = df.loc[df['split'] == 'val']\n",
    "val_ds = ProjectDataset.load_data_and_vectorizer(val_df, vectorizer_path)\n",
    "val_dl = DataLoader(val_ds, args.batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "test_df = df.loc[df['split'] == 'test']\n",
    "test_ds = ProjectDataset.load_data_and_vectorizer(test_df, vectorizer_path)\n",
    "test_dl = DataLoader(test_ds, args.batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T19:24:24.180252Z",
     "start_time": "2019-03-23T19:24:24.164486Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39200, 8400, 8400)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dl.dataset), len(val_dl.dataset), len(test_dl.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is required since Ignite takes only binary values for accuray computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T19:25:02.427454Z",
     "start_time": "2019-03-23T19:25:02.322806Z"
    }
   },
   "outputs": [],
   "source": [
    "def bce_logits_wrapper(output):\n",
    "    y_pred, y = output\n",
    "    y_pred = (torch.sigmoid(y_pred) > 0.5).long()\n",
    "    return y_pred, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T19:25:02.444359Z",
     "start_time": "2019-03-23T19:25:02.428896Z"
    }
   },
   "outputs": [],
   "source": [
    "classifier = Classifier(num_features=len((vectorizer).review_vocab))\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='min', factor=0.5, patience=1)\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "\n",
    "pbar = ProgressBar(persist=True)\n",
    "metrics = {'accuracy': Accuracy(bce_logits_wrapper), 'loss': Loss(loss_func)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T19:25:09.223253Z",
     "start_time": "2019-03-23T19:25:09.209095Z"
    }
   },
   "outputs": [],
   "source": [
    "args.num_epochs=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T19:25:45.097725Z",
     "start_time": "2019-03-23T19:25:09.224522Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/2]: [38/38] 100%|██████████, loss=6.47e-01 [00:06<00:00]\n",
      "Epoch [2/2]: [0/38]   0%|          , loss=5.57e-01 [00:00<?]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Training - Loss: 0.551, Accuracy: 0.831\n",
      "Validation - Loss: 0.558, Accuracy: 0.558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/2]: [38/38] 100%|██████████, loss=5.31e-01 [00:06<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n",
      "Training - Loss: 0.477, Accuracy: 0.851\n",
      "Validation - Loss: 0.487, Accuracy: 0.487\n"
     ]
    }
   ],
   "source": [
    "yelp_trainer = YelpTrainer(classifier, optimizer, loss_func, train_dl, val_dl, args, pbar, metrics)\n",
    "yelp_trainer.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T19:25:52.377416Z",
     "start_time": "2019-03-23T19:25:52.354159Z"
    }
   },
   "outputs": [],
   "source": [
    "classifier = Classifier(num_features=len((vectorizer).review_vocab))\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "\n",
    "if is_lite:\n",
    "  state_dict = torch.load(scratch/'yelp_classifier_lite.pth')\n",
    "else:\n",
    "  pass\n",
    "classifier.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ignite Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T19:25:54.860784Z",
     "start_time": "2019-03-23T19:25:54.846381Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluator = create_supervised_evaluator(classifier, metrics=metrics)\n",
    "\n",
    "@evaluator.on(Events.COMPLETED)\n",
    "def log_testing_results(engine):\n",
    "  metrics = engine.state.metrics\n",
    "  print(f\"Test loss: {metrics['loss']:0.3f}\")\n",
    "  print(f\"Test accuracy: {metrics['accuracy']:0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T19:25:57.639913Z",
     "start_time": "2019-03-23T19:25:55.705925Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.220\n",
      "Test accuracy: 0.918\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ignite.engine.engine.State at 0x7fb77e2c7f98>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.run(test_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLPBook Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T19:26:04.920118Z",
     "start_time": "2019-03-23T19:26:04.780118Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(y_pred, y):\n",
    "  y = y.type(torch.uint8)\n",
    "  y_pred = (torch.sigmoid(y_pred)>0.5)#.max(dim=1)[1]\n",
    "  n_correct = torch.eq(y_pred, y).sum().item()\n",
    "  return n_correct / len(y_pred) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T19:26:06.695103Z",
     "start_time": "2019-03-23T19:26:04.921663Z"
    }
   },
   "outputs": [],
   "source": [
    "running_loss = 0.\n",
    "running_acc = 0.\n",
    "\n",
    "classifier.eval()\n",
    "for i, batch in enumerate(test_dl):\n",
    "  x,y = batch\n",
    "  y_pred = classifier(x_in=x.float())\n",
    "  \n",
    "  loss = loss_func(y_pred, y.float())\n",
    "  loss_t = loss.item()\n",
    "  running_loss += (loss_t-running_loss)/(i+1)\n",
    "  \n",
    "  acc_t = compute_accuracy(y_pred, y)\n",
    "  running_acc += (acc_t-running_acc)/(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T19:26:06.720473Z",
     "start_time": "2019-03-23T19:26:06.697842Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.220\n",
      "Test acc: 91.785\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test loss: {running_loss:0.3f}\")\n",
    "print(f\"Test acc: {running_acc:0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T19:26:09.298270Z",
     "start_time": "2019-03-23T19:26:09.280941Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_rating(review, classifier, vectorizer, decision_threshold=0.5):\n",
    "  \"\"\"Predict the rating of a review\n",
    "\n",
    "  Args:\n",
    "      review (str): the text of the review\n",
    "      classifier (ReviewClassifier): the trained model\n",
    "      vectorizer (ReviewVectorizer): the corresponding vectorizer\n",
    "      decision_threshold (float): The numerical boundary which separates the rating classes\n",
    "  \"\"\"\n",
    "  review = preprocess_text(review)\n",
    "  print(review)\n",
    "\n",
    "  vectorized_review = torch.tensor(vectorizer.vectorize(review))\n",
    "  print(vectorized_review)\n",
    "  result = classifier(vectorized_review.view(1, -1))\n",
    "  print(result)\n",
    "\n",
    "  probability_value = torch.sigmoid(result).item()\n",
    "  print(probability_value)\n",
    "  index = 1\n",
    "  if probability_value < decision_threshold:\n",
    "      index = 0\n",
    "\n",
    "  return vectorizer.rating_vocab.lookup_idx(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T19:26:09.315501Z",
     "start_time": "2019-03-23T19:26:09.299770Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "while the begining of this book is great , the ending sucks\n",
      "tensor([1., 0., 0.,  ..., 0., 0., 0.])\n",
      "tensor([0.6867], grad_fn=<SqueezeBackward1>)\n",
      "0.6652267575263977\n",
      "While the begining of this book is great, the ending sucks -> positive\n"
     ]
    }
   ],
   "source": [
    "test_review = \"While the begining of this book is great, the ending sucks\"\n",
    "\n",
    "prediction = predict_rating(test_review, classifier, vectorizer, decision_threshold=0.5)\n",
    "print(f\"{test_review} -> {prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretablity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T19:26:09.330055Z",
     "start_time": "2019-03-23T19:26:09.316821Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7326])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fc1.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T19:26:09.345421Z",
     "start_time": "2019-03-23T19:26:09.331414Z"
    }
   },
   "outputs": [],
   "source": [
    "# sort weights\n",
    "fc1_weights = classifier.fc1.weight.detach()[0]\n",
    "_, idxs = torch.sort(fc1_weights, dim=0, descending=True)\n",
    "idxs = idxs.numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T19:26:09.361128Z",
     "start_time": "2019-03-23T19:26:09.346409Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Influential words in Positive Reviews:\n",
      "--------------------------------------\n",
      "delicious\n",
      "fantastic\n",
      "pleasantly\n",
      "amazing\n",
      "great\n",
      "ngreat\n",
      "excellent\n",
      "awesome\n",
      "bomb\n",
      "yum\n",
      "solid\n",
      "yummy\n",
      "perfect\n",
      "love\n",
      "pleased\n",
      "vegas\n",
      "amazed\n",
      "hooked\n",
      "legit\n",
      "delightful\n",
      "====\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Top 20 words\n",
    "print(\"Influential words in Positive Reviews:\")\n",
    "print(\"--------------------------------------\")\n",
    "for i in range(20):\n",
    "    print(vectorizer.review_vocab.lookup_idx(idxs[i]))\n",
    "    \n",
    "print(\"====\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T19:26:09.377087Z",
     "start_time": "2019-03-23T19:26:09.362202Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Influential words in Negative Reviews:\n",
      "--------------------------------------\n",
      "meh\n",
      "mediocre\n",
      "bland\n",
      "worst\n",
      "horrible\n",
      "slowest\n",
      "overpriced\n",
      "terrible\n",
      "nmaybe\n",
      "rude\n",
      "tasteless\n",
      "awful\n",
      "unfriendly\n",
      "inconsistent\n",
      "disappointing\n",
      "elsewhere\n",
      "underwhelmed\n",
      "poorly\n",
      "uninspired\n",
      "disgusting\n",
      "====\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Top 20 words\n",
    "print(\"Influential words in Negative Reviews:\")\n",
    "print(\"--------------------------------------\")\n",
    "idxs.reverse()\n",
    "for i in range(20):\n",
    "    print(vectorizer.review_vocab.lookup_idx(idxs[i]))\n",
    "    \n",
    "print(\"====\\n\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "208px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
